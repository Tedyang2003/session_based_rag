{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/python/3.12.7/lib/python3.12/site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/python/3.12.7/lib/python3.12/site-packages (1.20.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from onnx) (5.28.3)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/python/3.12.7/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/python/3.12.7/lib/python3.12/site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: packaging in /usr/local/python/3.12.7/lib/python3.12/site-packages (from onnxruntime) (24.2)\n",
      "Requirement already satisfied: sympy in /usr/local/python/3.12.7/lib/python3.12/site-packages (from onnxruntime) (1.13.3)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/python/3.12.7/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/python/3.12.7/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/rag_prototype/models/embedder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.08390327,  0.19795458,  0.29698628, ..., -0.0713546 ,\n",
       "          -0.09769778, -0.40413493],\n",
       "         [-0.1609579 ,  0.02804918,  0.15180624, ...,  0.09533711,\n",
       "           0.2472866 , -0.13007842],\n",
       "         [ 0.116238  , -0.04968553,  0.1525496 , ...,  0.13903278,\n",
       "           0.26824445, -0.32201058],\n",
       "         ...,\n",
       "         [ 0.24339935, -0.09559675,  0.07080201, ..., -0.04756254,\n",
       "           0.02960365, -0.6810834 ],\n",
       "         [ 0.16566315, -0.22281522,  0.14389402, ..., -0.04828335,\n",
       "           0.43901905, -0.37544316],\n",
       "         [ 0.07758234, -0.2386691 , -0.06030796, ..., -0.03946404,\n",
       "           0.15006924, -0.7060415 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"./gte-large/model/model.onnx\"  # Path to your ONNX model file\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)  # Verify the model's integrity\n",
    "\n",
    "# Create an ONNX runtime session\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Prepare dummy input (ensure the input shape matches the model's requirements)\n",
    "# For example, if you are using a BERT-like model, the typical input would be input_ids and attention_mask\n",
    "# You would need to tokenize your text and convert it to a numpy array before passing it to the model.\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"This is a test sentence.\"\n",
    "\n",
    "# You will need to tokenize the sentence, just as you would when using a PyTorch model\n",
    "# Here we use a simple placeholder (you should use the tokenizer for your specific model)\n",
    "# This is an example; in reality, you'll need to use the tokenizer from the Hugging Face library\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer (use the same tokenizer you used for model training)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./gte-large/tokenizer\")\n",
    "\n",
    "# Tokenize the input sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"np\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Prepare inputs for ONNX model (inputs should be numpy arrays)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "# Ensure the inputs are in the right format (numpy arrays)\n",
    "input_ids = np.array(input_ids, dtype=np.int64)\n",
    "attention_mask = np.array(attention_mask, dtype=np.int64)\n",
    "token_type_ids = np.array(token_type_ids, dtype=np.int64)\n",
    "# Run inference with ONNX Runtime\n",
    "# Set the input names as expected by the model (check the model's input names)\n",
    "# Here, 'input_ids' and 'attention_mask' are the input names\n",
    "outputs = session.run([\"last_hidden_state\"], {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    'token_type_ids': token_type_ids\n",
    "})\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
