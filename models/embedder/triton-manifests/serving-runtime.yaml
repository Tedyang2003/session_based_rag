apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-direct-s3-runtime
spec:
  containers:
    - args:
        - /bin/bash
        - -c
        - |
          pip install --no-cache-dir transformers && \
          tritonserver --model-repository=s3://minio-service.triton-inference-services.svc.cluster.local:9000/finalised-models/ensemble-repository --model-control-mode=poll --http-port=8080
      image: nvcr.io/nvidia/tritonserver:24.08-py3
      name: kserve-container
      ports:
        - name: http-port
          containerPort: 8080  # HTTP port for the inference server
        - name: grpc-port
          containerPort: 8001  # GRPC port for the inference server
        - name: metrics 
          containerPort: 8002  # Metrics port for the inference server
      env:
        - name: AWS_ACCESS_KEY_ID
          value: minio
        - name: AWS_SECRET_ACCESS_KEY
          value: minio123
        - name: AWS_DEFAULT_REGION
          value: us-east-1
        - name: HF_HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
        - name: HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
  multiModel: false
  protocolVersions:
    - v2
    - grpc-v2
  supportedModelFormats:
    - name: triton
      version: latest

