{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Onnx Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/rag_prototype/models/embedder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.08390327,  0.19795458,  0.29698628, ..., -0.0713546 ,\n",
       "          -0.09769778, -0.40413493],\n",
       "         [-0.1609579 ,  0.02804918,  0.15180624, ...,  0.09533711,\n",
       "           0.2472866 , -0.13007842],\n",
       "         [ 0.116238  , -0.04968553,  0.1525496 , ...,  0.13903278,\n",
       "           0.26824445, -0.32201058],\n",
       "         ...,\n",
       "         [ 0.24339935, -0.09559675,  0.07080201, ..., -0.04756254,\n",
       "           0.02960365, -0.6810834 ],\n",
       "         [ 0.16566315, -0.22281522,  0.14389402, ..., -0.04828335,\n",
       "           0.43901905, -0.37544316],\n",
       "         [ 0.07758234, -0.2386691 , -0.06030796, ..., -0.03946404,\n",
       "           0.15006924, -0.7060415 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"./gte-large/model/model.onnx\"  # Path to your ONNX model file\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)  # Verify the model's integrity\n",
    "\n",
    "# Create an ONNX runtime session\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Prepare dummy input (ensure the input shape matches the model's requirements)\n",
    "# For example, if you are using a BERT-like model, the typical input would be input_ids and attention_mask\n",
    "# You would need to tokenize your text and convert it to a numpy array before passing it to the model.\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"This is a test sentence.\"\n",
    "\n",
    "# You will need to tokenize the sentence, just as you would when using a PyTorch model\n",
    "# Here we use a simple placeholder (you should use the tokenizer for your specific model)\n",
    "# This is an example; in reality, you'll need to use the tokenizer from the Hugging Face library\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer (use the same tokenizer you used for model training)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./gte-large/tokenizer\")\n",
    "\n",
    "# Tokenize the input sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"np\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Prepare inputs for ONNX model (inputs should be numpy arrays)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "# Ensure the inputs are in the right format (numpy arrays)\n",
    "input_ids = np.array(input_ids, dtype=np.int64)\n",
    "attention_mask = np.array(attention_mask, dtype=np.int64)\n",
    "token_type_ids = np.array(token_type_ids, dtype=np.int64)\n",
    "# Run inference with ONNX Runtime\n",
    "# Set the input names as expected by the model (check the model's input names)\n",
    "# Here, 'input_ids' and 'attention_mask' are the input names\n",
    "outputs = session.run([\"last_hidden_state\"], {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    'token_type_ids': token_type_ids\n",
    "})\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Onnx Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# URL for the Triton service\n",
    "\n",
    "def emb_text(text, url=\"http://triton-direct-s3-route-triton-inference-services.apps.nebula.sl/v2/models/gte-base/infer\"):\n",
    "    \n",
    "    # Payload for the request\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"TEXT\",\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"shape\": [1],\n",
    "                \"data\": [text]\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Sending the POST request and printing the response\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        outputs = response.json()['outputs'][0]\n",
    "        data = outputs['data']\n",
    "        shape = outputs['shape']\n",
    "    \n",
    "        input_tensor =  np.array(data)\n",
    "    \n",
    "    \n",
    "        # Shape information from input tensor (this is provided in the 'shape' argument)\n",
    "        batch_size, seq_len, embedding_dim = shape\n",
    "    \n",
    "        # Reshape the 1D array into the original 3D shape (batch_size, seq_len, embedding_dim)\n",
    "        input_data = input_tensor.reshape(batch_size, seq_len, embedding_dim)\n",
    "    \n",
    "        # Apply mean pooling across the sequence length (axis=1)\n",
    "        pooled_embeddings = np.mean(input_data, axis=1)  # Shape will be [batch_size, embedding_dim]\n",
    "        \n",
    "        return pooled_embeddings.tolist()[0]\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Request failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_text('Hello what is your name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
